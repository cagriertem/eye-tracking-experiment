**Abstract**
In this study, we investigate the connection between photography and visual perception through the medium of Gestalt theory. Gestalt theory, which emphasizes the human mind’s innate ability to organize visual stimuli into structured and meaningful wholes, offers valuable insights into understanding how compositional elements in photography create unity, balance, and aesthetic appeal. Principles such as similarity, proximity, continuity, and figure-ground not only shape our interpretation of visual stimuli but also guide viewer attention in artistic compositions. While Gestalt principles have been extensively applied in design, architecture, and visual communication, their application to photographic arts remains relatively unexplored. Furthermore, there are a limited number of studies that investigate the impact of Gestalt properties through eye tracking experiments and deep learning models. Eye-tracking metrics, including fixations, saccades, and attention maps, are novel tools that provide extensive analyses of visual attention and perception. This study aims to bridge the gap between Gestalt theoretical principles and empirical evidence by exploring how Gestalt principles influence viewer attention and contribute to the aesthetic impact of photography. In other terms, our aim is to test the impact of Gestalt properties and investigate whether there is a significant difference between Gestalt properties in terms of certain eye tracking metrics. We claim that there is a significant difference between four Gestalt properties, namely, proximity, similarity, figure-ground and continuity. To prove this, in addition to statistical tests, we compare three different pre-trained deep learning models (ResNet50, EfficientNet and VGG16) that analyze the heat maps derived from eye tracking data. By combining computational methods with psychological theory, this research aims to give insights about the distinctive features of photographic composition that affect visual perception by providing a reasonable foundation for our understanding of the relationship between perception, aesthetics, and art. 

**Training**
We used three pre-trained deep learning models in the training, validation, and test phases, and compared the results. The deep learning models that we have used were ResNet50, EfficientNet, and VGG16. The models were trained using PyTorch library and the performance metrics were derived through Scikit-learn library in Python. The data used to train the models were derived via augmentation. We simply augmented the original dataset to reach a considerable and sufficient number of samples (3717 heat maps in total using 1252 heat maps initially) for training. The transformations include random horizontal flipping, brightness and contrast adjustments, Gaussian blur, posterization, elastic transformations, and sharpening, using PyTorch transform library. Before training the model, the training set was further transformed to prevent overfitting and acquire a better generalization on the dataset. These additional transformations include random horizontal flipping, random rotation within ±15 degrees, and adjustments to brightness, contrast, saturation, and hue.

**Results**
ResNet50, EfficientNet, and VGG16 on predicting Gestalt properties (continuity, figure-ground, similarity, and proximity) show various performances. ResNet50 achieved a test accuracy of 89.02%, with strong performance on continuity (precision: 0.96, recall: 0.86) and figure-ground (precision: 0.88, recall: 0.91). It had a solid ROC AUC score of 0.92, indicating good overall classification. Despite relatively high training and validation accuracies, performance varied slightly, especially in similarity (f1-score: 0.87, accuracy: 82.73%). EfficientNet, despite starting with lower accuracy, showed steady improvement, reaching a test accuracy of 87.54%. It demonstrated solid performance in precision for figure-ground, 0.93, and in recall for proximity, 0.92, but lower recall for similarity, 0.82. The model's ROC AUC score of 0.91 highlights its ability to discriminate across categories but slightly lagged behind ResNet50 in overall classification performance. VGG16 reached the highest test accuracy of 91.39%. It achieved strong scores in precision for figure-ground, 0.96, and in recall for continuity, 0.94, with high f-1 score across all categories. The model also demonstrated the best ROC AUC score of 0.94, indicating excellent model discrimination. The f1-scores were robust, particularly for figure-ground, 0.93, making it the most effective model. In conclusion, VGG16 outperformed the other two models in terms of overall accuracy, precision, recall, and AUC, while ResNet50 showed more balanced performance across the categories, and EfficientNet was slightly less effective but still competent.

All deep learning models predicted the Gestalt properties with high performance. However, VGG16 outperformed other two deep learning models, ResNet50 and EfficientNet. In the unseen test set, the models also perform well. VGG16 has a high AUC Score of 0.94 compared to ResNet50 with 0.92 and EfficientNet with 0.91. It is also worth noting that, for all three deep learning models, unfreezing the parameters of the last few layers improves the models’ performance. However, it is also worth noting that none of the three models exhibit smooth learning curves, likely due to the limited sample size. From all these results, we can infer that our hypothesis is supported as the models successfully predict the Gestalt properties using only a limited number of samples. Since such deep learning models require extensive datasets to train effectively and provide insights about complex patterns within the data, the results can be interpreted as promising and present a foundation for future studies.


